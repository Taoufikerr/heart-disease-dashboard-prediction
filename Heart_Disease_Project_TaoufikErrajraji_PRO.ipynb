{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f7efb6",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Project\n",
    "**Author: Taoufik Errajraji**\n",
    "\n",
    "This project analyzes a heart disease dataset to identify key health indicators associated with heart conditions. We perform statistical analysis, build machine learning and deep learning models, and evaluate their performance to predict the likelihood of heart disease in patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset structure and check for missing values or duplicates\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a44cb0",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms to understand the distribution of each feature\n",
    "df.hist(figsize=(12, 10), bins=20, edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c105f",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature relationships with the target variable\n",
    "import matplotlib.pyplot as plt\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    sns.countplot(data=df, x=col, hue='target')\n",
    "    plt.title(f'{col} vs Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4817a",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f363afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e77fa7",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfe366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = logreg.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Results\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Results\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network with Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0274558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43895f5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We developed and evaluated several models to predict heart disease using patient data. Both traditional machine learning models and deep learning approaches were tested. Future improvements could include hyperparameter tuning and using larger, more diverse datasets."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
